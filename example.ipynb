{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "tqdm.tqdm.pandas()\n",
    "import torch\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name ='epfl-dhlab/CatastroBERT'\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Make sure to move the model to the correct device (either 'cpu' or 'cuda')\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(text):\n",
    "    # Prepare the text data\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    ids = inputs['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mask = inputs['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(ids, mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Apply sigmoid function to get probabilities\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    # Return the probability of the class (1)\n",
    "    return probs[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to run individual tests to play around with the model, you can modify the text in the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"Un violent ouragan est passÃ© cette nuit sur Lausanne.\"\n",
    "print(f\"Prediction: {predict(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following definitions implement a simple torch dataset and a method to run inference on batches instead of a single example at a time. You can use this to run inference on a larger dataset and store the results in a file for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextInferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten()\n",
    "        }\n",
    "def create_data_loader(texts, tokenizer, batch_size=32, max_length=512):\n",
    "    dataset = TextInferenceDataset(texts, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Function to run inference\n",
    "def run_inference(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(data_loader, total=len(data_loader),desc='Inference'):\n",
    "            input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            predictions.extend(probs[:, 0])  # Assuming binary classification\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the batch size to fit your hardware constraints. If your dataset is large, I would advise to use gpu acceleration. If you don't have a gpu, you can use [Google Colab](https://colab.research.google.com/) to run the notebook on a gpu for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your data into a Pandas dataframe\n",
    "data = pd.read_csv('ressources/data.csv')\n",
    "\n",
    "#Step 2: set your batch size and Create a DataLoader for your text data \n",
    "batch_size = 512\n",
    "data_loader = create_data_loader(data['text'], tokenizer, batch_size, max_length=512)\n",
    "\n",
    "# Step 3: Run inference on your data\n",
    "predictions = run_inference(model, data_loader)\n",
    "predictions = [1 if x > 0.5 else 0 for x in predictions]\n",
    "predictions = pd.DataFrame({'summary':text, 'pred':predictions})\n",
    "\n",
    "# step 4: save your predictions\n",
    "predictions.to_csv('predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
